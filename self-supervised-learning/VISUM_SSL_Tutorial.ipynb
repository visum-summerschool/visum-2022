{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# https://urlis.net/5mj36  (on Discord as well)"
      ],
      "metadata": {
        "id": "qBduYjM-2e1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part0: Preparation\n"
      ],
      "metadata": {
        "id": "MWsLLdRX31q4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# download data\n",
        "_ = !wget http://www.robots.ox.ac.uk/~htd/misc/center_64x64_rgb.tar\n",
        "_ = !wget http://www.robots.ox.ac.uk/~htd/misc/center_64x64_flow.tar\n",
        "_ = !wget http://www.robots.ox.ac.uk/~htd/misc/train_split01.txt\n",
        "_ = !wget http://www.robots.ox.ac.uk/~htd/misc/test_split01.txt\n",
        "_ = !wget http://www.robots.ox.ac.uk/~htd/misc/ucf101_action.txt\n",
        "_ = !wget http://www.robots.ox.ac.uk/~htd/misc/golf_swing.tar\n",
        "_ = !wget http://www.robots.ox.ac.uk/~htd/misc/kitchen.tar"
      ],
      "metadata": {
        "id": "yXiwWBAH35mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_ = !tar xf center_64x64_rgb.tar\n",
        "_ = !tar xf center_64x64_flow.tar\n",
        "_ = !tar xf golf_swing.tar\n",
        "_ = !tar xf kitchen.tar"
      ],
      "metadata": {
        "id": "a4gnb4Tg64zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gBZM9-ZFJDRg",
        "outputId": "d1e739ed-fcae-413e-b482-44cefebf811d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "center_64x64_flow      golf_swing\t train_split01.txt\n",
            "center_64x64_flow.tar  golf_swing.tar\t ucf101_action.txt\n",
            "center_64x64_rgb       sample_data\n",
            "center_64x64_rgb.tar   test_split01.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part1: Multi-modality in video: optical flow\n",
        "Video naturally comes with multiple modalities (thinking of YouTube videos for example): RGB frames, audio, and text (video titles, tags, speech subtitles, comments). In this part we introduce a unique modality in video data: **optical flow**.\n",
        "\n",
        "[Wikipedia](https://en.wikipedia.org/wiki/Optical_flow): Optical flow or optic flow is the pattern of apparent motion of objects, surfaces, and edges in a visual scene caused by the relative motion between an observer and a scene.\n",
        "\n",
        "It is shown to be a complementary information with the RGB stream: [Two-Stream Convolutional Networks\n",
        "for Action Recognition in Videos](https://proceedings.neurips.cc/paper/2014/file/00ec53c4682d36f5c4359f4ae7bd7ba1-Paper.pdf)\n",
        "\n",
        "Let's take a closer look with examples!"
      ],
      "metadata": {
        "id": "HQ1DnzmQ3WB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def compute_TVL1(prev, curr):\n",
        "    \"\"\"Compute the TV-L1 optical flow. https://docs.opencv.org/4.x/dc/d4d/classcv_1_1optflow_1_1DualTVL1OpticalFlow.html \"\"\"\n",
        "    TVL1 = cv2.optflow.DualTVL1OpticalFlow_create()\n",
        "    flow = TVL1.calc(prev, curr, None)\n",
        "    return flow"
      ],
      "metadata": {
        "id": "oyr3gvZP75Gh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls golf_swing/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wR1rbTqAG0rQ",
        "outputId": "c0a46ea8-f78e-4ca7-f85f-41500b5d999f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1-00035.jpg  1-00037.jpg  2-00045.jpg  2-00047.jpg  3-00082.jpg  3-00084.jpg\n",
            "1-00036.jpg  1-00038.jpg  2-00046.jpg  2-00048.jpg  3-00083.jpg  3-00085.jpg\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prev = cv2.imread('golf_swing/3-00082.jpg')\n",
        "prev = cv2.cvtColor(prev, cv2.COLOR_BGR2GRAY)\n",
        "curr = cv2.imread('golf_swing/3-00083.jpg')\n",
        "curr = cv2.cvtColor(curr, cv2.COLOR_BGR2GRAY)\n",
        "flow = compute_TVL1(prev, curr)\n",
        "\n",
        "# Q1: what is the shape of the optical flow?\n",
        "# Q2: how would you visualize it?"
      ],
      "metadata": {
        "id": "sl8Wqu6K8ypa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# a recent visualization method: https://github.com/princeton-vl/RAFT/blob/master/core/utils/flow_viz.py "
      ],
      "metadata": {
        "id": "pAKPjTYgfGFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part2: SSL preliminary"
      ],
      "metadata": {
        "id": "1xjImdWO9pHm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1. Import modules"
      ],
      "metadata": {
        "id": "LYs2Telc_MjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpu_info = !nvidia-smi -i 0\n",
        "gpu_info = '\\n'.join(gpu_info)\n",
        "print(gpu_info)\n",
        "\n",
        "from datetime import datetime\n",
        "from functools import partial\n",
        "from PIL import Image\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet\n",
        "from tqdm import tqdm\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FHQh5qkL_L5j",
        "outputId": "1af19029-8983-4817-f57a-40c5df16bcb1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul 12 08:57:58 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P8     9W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.2. Set arguments"
      ],
      "metadata": {
        "id": "hYsFFsWHFEov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "parser = argparse.ArgumentParser(description='Train MoCo on CIFAR-10')\n",
        "\n",
        "parser.add_argument('-a', '--arch', default='resnet18')\n",
        "\n",
        "# lr: 0.06 for batch 512 (or 0.03 for batch 256)\n",
        "parser.add_argument('--lr', '--learning-rate', default=0.06, type=float, metavar='LR', help='initial learning rate', dest='lr')\n",
        "parser.add_argument('--epochs', default=10, type=int, metavar='N', help='number of total epochs to run')\n",
        "parser.add_argument('--schedule', default=[120, 160], nargs='*', type=int, help='learning rate schedule (when to drop lr by 10x); does not take effect if --cos is on')\n",
        "parser.add_argument('--cos', action='store_true', help='use cosine lr schedule')\n",
        "\n",
        "parser.add_argument('--batch-size', default=256, type=int, metavar='N', help='mini-batch size')\n",
        "parser.add_argument('--wd', default=5e-4, type=float, metavar='W', help='weight decay')\n",
        "\n",
        "# moco specific configs:\n",
        "parser.add_argument('--moco-dim', default=128, type=int, help='feature dimension')\n",
        "parser.add_argument('--moco-k', default=4096, type=int, help='queue size; number of negative keys')\n",
        "parser.add_argument('--moco-m', default=0.99, type=float, help='moco momentum of updating key encoder')\n",
        "parser.add_argument('--moco-t', default=0.07, type=float, help='softmax temperature')\n",
        "\n",
        "parser.add_argument('--bn-splits', default=8, type=int, help='simulate multi-gpu behavior of BatchNorm in one gpu; 1 is SyncBatchNorm in multi-gpu')\n",
        "\n",
        "parser.add_argument('--symmetric', action='store_true', help='use a symmetric loss function that backprops to both crops')\n",
        "\n",
        "# knn monitor\n",
        "parser.add_argument('--knn-k', default=10, type=int, help='k in kNN monitor')\n",
        "parser.add_argument('--knn-t', default=0.07, type=float, help='softmax temperature in kNN monitor; could be different with moco-t')\n",
        "\n",
        "# utils\n",
        "parser.add_argument('--resume', default='', type=str, metavar='PATH', help='path to latest checkpoint (default: none)')\n",
        "parser.add_argument('--results-dir', default='', type=str, metavar='PATH', help='path to cache (default: none)')\n",
        "\n",
        "'''\n",
        "args = parser.parse_args()  # running in command line\n",
        "'''\n",
        "args = parser.parse_args('')  # running in ipynb\n",
        "\n",
        "# set command line arguments here when running in ipynb\n",
        "args.epochs = 200\n",
        "args.cos = True\n",
        "args.schedule = []  # cos in use\n",
        "args.symmetric = False\n",
        "if args.results_dir == '':\n",
        "    args.results_dir = './cache-' + datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S-ssl\")\n",
        "\n",
        "print(args)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZWaGBu4FKri",
        "outputId": "a3453578-abad-47a3-8a3f-0305f2e6b113"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Namespace(arch='resnet18', batch_size=256, bn_splits=8, cos=True, epochs=200, knn_k=10, knn_t=0.07, lr=0.06, moco_dim=128, moco_k=4096, moco_m=0.99, moco_t=0.07, results_dir='./cache-2022-07-12-09-26-21-ssl', resume='', schedule=[], symmetric=False, wd=0.0005)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3. Define data loaders"
      ],
      "metadata": {
        "id": "pl-a8X5f_Wx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_txt(path):\n",
        "  with open(path) as fobj:\n",
        "    content = fobj.readlines()\n",
        "    return [i.rstrip() for i in content]\n",
        "\n",
        "def pil_loader(path):\n",
        "  with open(path, 'rb') as f:\n",
        "    with Image.open(f) as img:\n",
        "      return img.convert('RGB')\n",
        "\n",
        "\n",
        "class MiniUCF101Pair():\n",
        "    \"\"\"Mini-version of UCF101 Dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, root, train=False, transform=None):\n",
        "      self.root = root\n",
        "      self.train = train\n",
        "      self.transform = transform\n",
        "      if train:\n",
        "        self.videolist = load_txt('train_split01.txt')\n",
        "      else:\n",
        "        self.videolist = load_txt('test_split01.txt')\n",
        "      action_list = load_txt('ucf101_action.txt')\n",
        "      self.act2idx = {act:i for i,act in enumerate(action_list)}\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.videolist)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        vname = self.videolist[index]\n",
        "        img = pil_loader(os.path.join(self.root, f'{vname}.jpg'))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            im_1 = self.transform(img)\n",
        "            im_2 = self.transform(img)\n",
        "\n",
        "        return im_1, im_2\n",
        "\n",
        "\n",
        "class MiniUCF101():\n",
        "    \"\"\"Mini-version of UCF101 Dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, root, train=False, transform=None):\n",
        "      self.root = root\n",
        "      self.train = train\n",
        "      self.transform = transform\n",
        "      if train:\n",
        "        self.videolist = load_txt('train_split01.txt')\n",
        "      else:\n",
        "        self.videolist = load_txt('test_split01.txt')\n",
        "      action_list = load_txt('ucf101_action.txt')\n",
        "      self.act2idx = {act:i for i,act in enumerate(action_list)}\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.videolist)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        vname = self.videolist[index]\n",
        "        img = pil_loader(os.path.join(self.root, f'{vname}.jpg'))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            im = self.transform(img)\n",
        "\n",
        "        label = self.act2idx[vname.split('/')[0]]\n",
        "        return im, label\n",
        "\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "# data prepare\n",
        "train_data = MiniUCF101Pair(root='center_64x64_rgb', train=True, transform=train_transform)\n",
        "train_loader = DataLoader(train_data, batch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "\n",
        "memory_data = MiniUCF101(root='center_64x64_rgb', train=True, transform=test_transform)\n",
        "memory_loader = DataLoader(memory_data, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "test_data = MiniUCF101(root='center_64x64_rgb', train=False, transform=test_transform)\n",
        "test_loader = DataLoader(test_data, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "JSoS6D7j_V-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3.1. (Optional): can you visualize these augmented images?"
      ],
      "metadata": {
        "id": "th-SskVDlLx-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vname = train_data.videolist[0]\n",
        "img = pil_loader(os.path.join(train_data.root, f'{vname}.jpg'))\n",
        "for _ in range(10):\n",
        "  aug_img = train_transform(img)\n",
        "  # visualize?"
      ],
      "metadata": {
        "id": "52Es_t46lMyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.4. Define base encoder"
      ],
      "metadata": {
        "id": "-STUlAwREkEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ModelBase(nn.Module):\n",
        "  \"\"\"\n",
        "  Common CIFAR ResNet recipe.\n",
        "  Comparing with ImageNet ResNet recipe, it:\n",
        "  (i) replaces conv1 with kernel=3, str=1\n",
        "  (ii) removes pool1\n",
        "  \"\"\"\n",
        "  def __init__(self, feature_dim=128, arch=None, bn_splits=16):\n",
        "      super(ModelBase, self).__init__()\n",
        "\n",
        "      # use split batchnorm\n",
        "      # norm_layer = partial(SplitBatchNorm, num_splits=bn_splits) if bn_splits > 1 else nn.BatchNorm2d\n",
        "      norm_layer = nn.BatchNorm2d\n",
        "      resnet_arch = getattr(resnet, arch)\n",
        "      net = resnet_arch(num_classes=feature_dim, norm_layer=norm_layer)\n",
        "\n",
        "      self.net = []\n",
        "      for name, module in net.named_children():\n",
        "          if name == 'conv1':\n",
        "              module = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "          if isinstance(module, nn.MaxPool2d):\n",
        "              continue\n",
        "          if isinstance(module, nn.Linear):\n",
        "              self.net.append(nn.Flatten(1))\n",
        "          self.net.append(module)\n",
        "\n",
        "      self.net = nn.Sequential(*self.net)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.net(x)\n",
        "      # note: not normalized here\n",
        "      return x"
      ],
      "metadata": {
        "id": "9JPHYeCrEjQ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5. Define info-NCE wrapper"
      ],
      "metadata": {
        "id": "u-VxqLJBGugc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"http://www.robots.ox.ac.uk/~htd/misc/info-nce.png\" width=\"400\"/> \n",
        "\n",
        "Reference: [CPC](https://arxiv.org/abs/1807.03748), [SimCLR](https://arxiv.org/abs/2002.05709), [MoCo](https://arxiv.org/abs/1911.05722)."
      ],
      "metadata": {
        "id": "eMkxdP_NmSHE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InfoNCE(ModelBase):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__(*args, **kwargs)\n",
        "  \n",
        "  def forward(self, x1, x2):\n",
        "    feature1 = self.net(x1)  # BxC\n",
        "    feature2 = self.net(x2)  # BxC\n",
        "    \n",
        "    # define info-NCE loss here: you can use temperature 0.07\n",
        "    loss = None\n",
        "    \n",
        "    return loss\n",
        "\n",
        "model = InfoNCE(arch='resnet18')"
      ],
      "metadata": {
        "id": "nKV4AMZvGx-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6. Define train/test"
      ],
      "metadata": {
        "id": "Co5S4JOkGVYL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# train for one epoch\n",
        "def train(net, data_loader, train_optimizer, epoch, args):\n",
        "    net.train()\n",
        "    adjust_learning_rate(optimizer, epoch, args)\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "\n",
        "    total_loss, total_num, train_bar = 0.0, 0, tqdm(data_loader)\n",
        "    for im_1, im_2 in train_bar:\n",
        "      if use_cuda:\n",
        "        im_1, im_2 = im_1.cuda(non_blocking=True), im_2.cuda(non_blocking=True)\n",
        "\n",
        "      loss = net(im_1, im_2)\n",
        "      train_optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      train_optimizer.step()\n",
        "\n",
        "      total_num += data_loader.batch_size\n",
        "      total_loss += loss.item() * data_loader.batch_size\n",
        "      train_bar.set_description('Train Epoch: [{}/{}], lr: {:.6f}, Loss: {:.4f}'.format(epoch, args.epochs, optimizer.param_groups[0]['lr'], total_loss / total_num))\n",
        "\n",
        "    return total_loss / total_num\n",
        "\n",
        "# lr scheduler for training\n",
        "def adjust_learning_rate(optimizer, epoch, args):\n",
        "    \"\"\"Decay the learning rate based on schedule\"\"\"\n",
        "    lr = args.lr\n",
        "    if args.cos:  # cosine lr schedule\n",
        "        lr *= 0.5 * (1. + math.cos(math.pi * epoch / args.epochs))\n",
        "    else:  # stepwise lr schedule\n",
        "        for milestone in args.schedule:\n",
        "            lr *= 0.1 if epoch >= milestone else 1.\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ],
      "metadata": {
        "id": "1N1CpqEPGZe4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test using a knn monitor\n",
        "def test(net, memory_data_loader, test_data_loader, epoch, args):\n",
        "    net.eval()\n",
        "    classes = len(memory_data_loader.dataset.act2idx)\n",
        "    total_top1, total_top5, total_num, feature_bank = 0.0, 0.0, 0, []\n",
        "    bank_label = []\n",
        "    with torch.no_grad():\n",
        "        # generate feature bank\n",
        "        for data, target in tqdm(memory_data_loader, desc='Feature extracting'):\n",
        "            feature = net(data.cuda(non_blocking=True))\n",
        "            feature = F.normalize(feature, dim=1)\n",
        "            feature_bank.append(feature)\n",
        "            bank_label.append(target)\n",
        "        # [D, N]\n",
        "        feature_bank = torch.cat(feature_bank, dim=0).t().contiguous()\n",
        "        feature_labels = torch.cat(bank_label, dim=0).to(feature_bank.device)\n",
        "\n",
        "        # [N]\n",
        "        # feature_labels = torch.tensor(memory_data_loader.dataset.targets, device=feature_bank.device)\n",
        "        # loop test data to predict the label by weighted knn search\n",
        "        test_bar = tqdm(test_data_loader)\n",
        "        for data, target in test_bar:\n",
        "            data, target = data.cuda(non_blocking=True), target.cuda(non_blocking=True)\n",
        "            feature = net(data)\n",
        "            feature = F.normalize(feature, dim=1)\n",
        "            \n",
        "            pred_labels = knn_predict(feature, feature_bank, feature_labels, classes, args.knn_k, args.knn_t)\n",
        "\n",
        "            total_num += data.size(0)\n",
        "            total_top1 += (pred_labels[:, 0] == target).float().sum().item()\n",
        "            test_bar.set_description('Test Epoch: [{}/{}] Acc@1:{:.2f}%'.format(epoch, args.epochs, total_top1 / total_num * 100))\n",
        "\n",
        "    return total_top1 / total_num * 100\n",
        "\n",
        "\n",
        "# knn monitor as in InstDisc https://arxiv.org/abs/1805.01978\n",
        "# implementation follows http://github.com/zhirongw/lemniscate.pytorch and https://github.com/leftthomas/SimCLR\n",
        "def knn_predict(feature, feature_bank, feature_labels, classes, knn_k, knn_t):\n",
        "    # compute cos similarity between each feature vector and feature bank ---> [B, N]\n",
        "    sim_matrix = torch.mm(feature, feature_bank)\n",
        "    # [B, K]\n",
        "    sim_weight, sim_indices = sim_matrix.topk(k=knn_k, dim=-1)\n",
        "    # [B, K]\n",
        "    sim_labels = torch.gather(feature_labels.expand(feature.size(0), -1), dim=-1, index=sim_indices)\n",
        "    sim_weight = (sim_weight / knn_t).exp()\n",
        "\n",
        "    # counts for each class\n",
        "    one_hot_label = torch.zeros(feature.size(0) * knn_k, classes, device=sim_labels.device)\n",
        "    # [B*K, C]\n",
        "    one_hot_label = one_hot_label.scatter(dim=-1, index=sim_labels.view(-1, 1), value=1.0)\n",
        "    # weighted score ---> [B, C]\n",
        "    pred_scores = torch.sum(one_hot_label.view(feature.size(0), -1, classes) * sim_weight.unsqueeze(dim=-1), dim=1)\n",
        "\n",
        "    pred_labels = pred_scores.argsort(dim=-1, descending=True)\n",
        "    return pred_labels"
      ],
      "metadata": {
        "id": "HVyrYSHygyk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7. Start training"
      ],
      "metadata": {
        "id": "gWZnptcoJutf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  model.cuda()\n",
        "\n",
        "# define optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=args.lr, weight_decay=args.wd, momentum=0.9)\n",
        "\n",
        "# load model if resume\n",
        "epoch_start = 1\n",
        "if args.resume is not '':\n",
        "    checkpoint = torch.load(args.resume)\n",
        "    model.load_state_dict(checkpoint['state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    epoch_start = checkpoint['epoch'] + 1\n",
        "    print('Loaded from: {}'.format(args.resume))\n",
        "\n",
        "# logging\n",
        "results = {'train_loss': [], 'test_acc@1': []}\n",
        "if not os.path.exists(args.results_dir):\n",
        "    os.mkdir(args.results_dir)\n",
        "# dump args\n",
        "with open(args.results_dir + '/args.json', 'w') as fid:\n",
        "    json.dump(args.__dict__, fid, indent=2)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(epoch_start, args.epochs + 1):\n",
        "    train_loss = train(model, train_loader, optimizer, epoch, args)\n",
        "    results['train_loss'].append(train_loss)\n",
        "    test_acc_1 = test(model.net, memory_loader, test_loader, epoch, args)\n",
        "    results['test_acc@1'].append(test_acc_1)\n",
        "    # save statistics\n",
        "    data_frame = pd.DataFrame(data=results, index=range(epoch_start, epoch + 1))\n",
        "    data_frame.to_csv(args.results_dir + '/log.csv', index_label='epoch')\n",
        "    # save model\n",
        "    torch.save({'epoch': epoch, 'state_dict': model.state_dict(), 'optimizer' : optimizer.state_dict(),}, args.results_dir + '/model_last.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 506
        },
        "id": "Z0OaGFraJw_0",
        "outputId": "d367980f-f6e9-4230-a681-8344363f90e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Train Epoch: [1/200], lr: 0.059996, Loss: 3.1775: 100%|██████████| 37/37 [00:20<00:00,  1.84it/s]\n",
            "Feature extracting: 100%|██████████| 38/38 [00:04<00:00,  9.27it/s]\n",
            "Test Epoch: [1/200] Acc@1:16.98%: 100%|██████████| 15/15 [00:02<00:00,  7.47it/s]\n",
            "Train Epoch: [2/200], lr: 0.059985, Loss: 3.0112: 100%|██████████| 37/37 [00:19<00:00,  1.88it/s]\n",
            "Feature extracting: 100%|██████████| 38/38 [00:04<00:00,  9.46it/s]\n",
            "Test Epoch: [2/200] Acc@1:17.16%: 100%|██████████| 15/15 [00:01<00:00,  7.62it/s]\n",
            "Train Epoch: [3/200], lr: 0.059967, Loss: 2.7908:  30%|██▉       | 11/37 [00:07<00:17,  1.47it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-f14297ef27fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# training loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mtrain_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'train_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mtest_acc_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-11-aee3235e0085>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(net, data_loader, train_optimizer, epoch, args)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0mtotal_num\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m       \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m       \u001b[0mtrain_bar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_description\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Train Epoch: [{}/{}], lr: {:.6f}, Loss: {:.4f}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_groups\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'lr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_num\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part3: Interesting findings on Optical flow"
      ],
      "metadata": {
        "id": "ACp1inNiK5YX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall your memory from Part1, do you think the optical flow from different golf-swing videos are similar?\n",
        "How do we benefit from this property?\n",
        "\n",
        "First, we can train an InfoNCE model on optical flow as well."
      ],
      "metadata": {
        "id": "5QVNnP4uLDFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1. Prepare optical flow loader"
      ],
      "metadata": {
        "id": "aqwC3XWrKiMP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# data prepare\n",
        "flow_train_data = MiniUCF101Pair(root='center_64x64_flow', train=True, transform=train_transform)\n",
        "flow_train_loader = DataLoader(flow_train_data, batch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "\n",
        "flow_memory_data = MiniUCF101(root='center_64x64_flow', train=True, transform=test_transform)\n",
        "flow_memory_loader = DataLoader(flow_memory_data, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "flow_test_data = MiniUCF101(root='center_64x64_flow', train=False, transform=test_transform)\n",
        "flow_test_loader = DataLoader(flow_test_data, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "q9FE3hF4LDzd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2. Train the info-NCE for optical flow"
      ],
      "metadata": {
        "id": "WGvpHsUFKmq1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "flow_model = InfoNCE(arch='resnet18')\n",
        "\n",
        "# define optimizer\n",
        "flow_optimizer = torch.optim.SGD(flow_model.parameters(), lr=args.lr, weight_decay=args.wd, momentum=0.9)\n",
        "\n",
        "# load model if resume\n",
        "epoch_start = 1\n",
        "if args.resume is not '':\n",
        "    checkpoint = torch.load(args.resume)\n",
        "    flow_model.load_state_dict(checkpoint['state_dict'])\n",
        "    flow_optimizer.load_state_dict(checkpoint['optimizer'])\n",
        "    epoch_start = checkpoint['epoch'] + 1\n",
        "    print('Loaded from: {}'.format(args.resume))\n",
        "\n",
        "# logging\n",
        "results = {'train_loss': [], 'test_acc@1': []}\n",
        "if not os.path.exists(args.results_dir):\n",
        "    os.mkdir(args.results_dir)\n",
        "# dump args\n",
        "with open(args.results_dir + '/args.json', 'w') as fid:\n",
        "    json.dump(args.__dict__, fid, indent=2)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(epoch_start, args.epochs + 1):\n",
        "    train_loss = train(flow_model, flow_train_loader, flow_optimizer, epoch, args)\n",
        "    results['train_loss'].append(train_loss)\n",
        "    test_acc_1 = test(model.net, memory_loader, test_loader, epoch, args)\n",
        "    results['test_acc@1'].append(test_acc_1)\n",
        "    # save statistics\n",
        "    data_frame = pd.DataFrame(data=results, index=range(epoch_start, epoch + 1))\n",
        "    data_frame.to_csv(args.results_dir + '/log.csv', index_label='epoch')\n",
        "    # save model\n",
        "    torch.save({'epoch': epoch, 'state_dict': flow_model.state_dict(), 'optimizer' : flow_optimizer.state_dict(),}, args.results_dir + '/flow_model_last.pth')"
      ],
      "metadata": {
        "id": "h6VpHDrxJq0P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part4: Self-supervised Co-Traininig"
      ],
      "metadata": {
        "id": "DOLjVIiRKWLc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspired by our observations from the optical flow, we think we can use optical flow model to \"teach\" RGB model to learn better, and vice versa. Still, everything is self-supervised (no action label is used for training).\n",
        "\n",
        "This is also the key idea of our NeurIPS2020 paper: \n",
        "\n",
        "[Self-supervised Co-training for Video Representation Learning](https://www.robots.ox.ac.uk/~vgg/research/CoCLR/)"
      ],
      "metadata": {
        "id": "0NPUO25dK1XG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www.robots.ox.ac.uk/~vgg/research/CoCLR/assets/images/coclr_teaser.png\" width=\"500\"/>"
      ],
      "metadata": {
        "id": "ZjAxevSSYrjv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1. Prepare two-stream loader"
      ],
      "metadata": {
        "id": "4dc0QIoVL20Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MiniUCF101_2Stream_Pair():\n",
        "    \"\"\"Mini-version of UCF101 Dataset.\n",
        "    \"\"\"\n",
        "    def __init__(self, root_rgb, root_flow, train=False, transform=None):\n",
        "      self.root_rgb = root_rgb\n",
        "      self.root_flow = root_flow\n",
        "      self.train = train\n",
        "      self.transform = transform\n",
        "      if train:\n",
        "        self.videolist = load_txt('train_split01.txt')\n",
        "      else:\n",
        "        self.videolist = load_txt('test_split01.txt')\n",
        "      action_list = load_txt('ucf101_action.txt')\n",
        "      self.act2idx = {act:i for i,act in enumerate(action_list)}\n",
        "\n",
        "    def __len__(self):\n",
        "      return len(self.videolist)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        vname = self.videolist[index]\n",
        "        img_rgb = pil_loader(os.path.join(self.root, f'{vname}.jpg'))\n",
        "        img_flow = pil_loader(os.path.join(self.root, f'{vname}.jpg'))\n",
        "\n",
        "        if self.transform is not None:\n",
        "            im_1 = self.transform(img_rgb)\n",
        "            im_2 = self.transform(img_rgb)\n",
        "\n",
        "            flow_1 = self.transform(img_flow)\n",
        "            flow_2 = self.transform(img_flow)\n",
        "\n",
        "        return im_1, im_2, flow_1, flow_2\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.RandomResizedCrop(32),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomApply([transforms.ColorJitter(0.4, 0.4, 0.4, 0.1)], p=0.8),\n",
        "    transforms.RandomGrayscale(p=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])])\n",
        "\n",
        "# data prepare\n",
        "train_data_2s = MiniUCF101_2Stream_Pair(root_rgb='center_64x64_rgb', root_flow='center_64x64_flow', train=True, transform=train_transform)\n",
        "train_loader_2s = DataLoader(train_data_2s, batch_size=args.batch_size, shuffle=True, num_workers=2, pin_memory=True, drop_last=True)\n",
        "\n",
        "test_data_2s = MiniUCF101_2Stream_Pair(root='center_64x64_rgb', root_flow='center_64x64_flow', train=False, transform=test_transform)\n",
        "test_loader_2s = DataLoader(test_data_2s, batch_size=args.batch_size, shuffle=False, num_workers=2, pin_memory=True)"
      ],
      "metadata": {
        "id": "P8KS__M0KZer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2. Define Co-Training wrapper"
      ],
      "metadata": {
        "id": "R6_DC91rMvGz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CoTraining(nn.Module):\n",
        "  def __init__(self, *args, **kwargs):\n",
        "    super().__init__()\n",
        "    self.rgb_net = ModelBase(*args, **kwargs)\n",
        "    self.flow_net = ModelBase(*args, **kwargs)\n",
        "  \n",
        "  def forward(self, x1, x2, f1, f2):\n",
        "    rgb_feature1 = self.rgb_net.net(x1)  # BxC\n",
        "    rgb_feature2 = self.rgb_net.net(x2)  # BxC\n",
        "    flow_feature1 = self.flow_net.net(f1)  # BxC\n",
        "    flow_feature2 = self.flow_net.net(f2)  # BxC\n",
        "        \n",
        "    rgb_sim = F.normalize(rgb_feature1, dim=1).matmul(F.normalize(rgb_feature2, dim=1).T).div(0.07)\n",
        "    flow_sim = F.normalize(flow_feature1, dim=1).matmul(F.normalize(flow_feature2, dim=1).T).div(0.07)\n",
        "    \n",
        "    # define info-NCE loss here\n",
        "    loss = None\n",
        "\n",
        "    return loss\n",
        "\n",
        "model = CoTraining(arch='resnet18')"
      ],
      "metadata": {
        "id": "exzqKCkmMxfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3. (Optional) Co-Train rgb and flow networks"
      ],
      "metadata": {
        "id": "tMDT6YXdsykC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# define model\n",
        "cotrain_model = CoTraining(arch='resnet18')\n",
        "\n",
        "# define optimizer\n",
        "cotrain_optimizer = torch.optim.SGD(cotrain_model.parameters(), lr=args.lr, weight_decay=args.wd, momentum=0.9)\n",
        "\n",
        "# load model if resume: \n",
        "epoch_start = 1\n",
        "rgb_ckpt = ''\n",
        "flow_ckpt = ''\n",
        "\n",
        "rgb_checkpoint = torch.load(rgb_ckpt)['state_dict']\n",
        "flow_checkpoint = torch.load(flow_ckpt)['state_dict']\n",
        "combined_ckpt = {}\n",
        "for k,v in rgb_checkpoint.items():\n",
        "  combined_ckpt[f'rgb_net.{k}'] = v\n",
        "for k,v in flow_checkpoint.items():\n",
        "  combined_ckpt[f'flow_net.{k}'] = v\n",
        "cotrain_model.load_state_dict(combined_ckpt)\n",
        "print('Loaded from: {} and {}'.format(rgb_ckpt, flow_ckpt))\n",
        "\n",
        "# logging\n",
        "results = {'train_loss': [], 'test_acc@1': []}\n",
        "if not os.path.exists(args.results_dir):\n",
        "    os.mkdir(args.results_dir)\n",
        "# dump args\n",
        "with open(args.results_dir + '/args.json', 'w') as fid:\n",
        "    json.dump(args.__dict__, fid, indent=2)\n",
        "\n",
        "# training loop\n",
        "for epoch in range(epoch_start, args.epochs + 1):\n",
        "    train_loss = train(cotrain_model, train_loader_2s, cotrain_optimizer, epoch, args)\n",
        "    results['train_loss'].append(train_loss)\n",
        "    test_acc_1 = test(model.rgb_net.net, memory_loader, test_loader, epoch, args)\n",
        "    results['test_acc@1'].append(test_acc_1)\n",
        "    # save statistics\n",
        "    data_frame = pd.DataFrame(data=results, index=range(epoch_start, epoch + 1))\n",
        "    data_frame.to_csv(args.results_dir + '/log.csv', index_label='epoch')\n",
        "    # save model\n",
        "    torch.save({'epoch': epoch, 'state_dict': flow_model.state_dict(), 'optimizer' : flow_optimizer.state_dict(),}, args.results_dir + '/cotrain_model_last.pth')"
      ],
      "metadata": {
        "id": "MwQ5tpV3s__h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4. Our paper and related works."
      ],
      "metadata": {
        "id": "_yTNg50dwP4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "More details in our [paper](https://www.robots.ox.ac.uk/~vgg/research/CoCLR/):\n",
        "* Memory bank to increase batch size\n",
        "* Iteratively optimize two networks\n",
        "* Pre-train on larger datasets\n",
        "\n",
        "Other related works in this line of research: [SupportSet](https://arxiv.org/abs/2010.02824), [SwAV](https://arxiv.org/abs/2006.09882)\n"
      ],
      "metadata": {
        "id": "rBjW8zK8wZc1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhAozo7ASabf"
      },
      "source": [
        "# Conclusion and Limitation"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Familar with optical flow in videos\n",
        "* Understand classic SSL algorithm\n",
        "* Multi-modal SSL algorithm\n",
        "* Finally, an informal introduction of multi-modal SSL research now: [link](https://twitter.com/osanseviero/status/1546224159553986561?s=20&t=rYbQ2MWKaAPiZ13MUTxPBQ)\n",
        "\n",
        "<img src=\"http://www.robots.ox.ac.uk/~htd/misc/gpu_figure.png\" width=\"200\"/>\n"
      ],
      "metadata": {
        "id": "nxDuBNTaNrin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "JtWmMrtVNTpZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "VISUM-SSL-Tutorial",
      "provenance": [],
      "collapsed_sections": [
        "HQ1DnzmQ3WB_",
        "LYs2Telc_MjW",
        "pl-a8X5f_Wx5",
        "th-SskVDlLx-",
        "-STUlAwREkEw",
        "Co5S4JOkGVYL",
        "gWZnptcoJutf",
        "4dc0QIoVL20Y",
        "R6_DC91rMvGz"
      ]
    },
    "environment": {
      "name": "pytorch-gpu.1-4.m50",
      "type": "gcloud",
      "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}